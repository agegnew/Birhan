{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "voice command.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiHRn3/srGNJkQG2FLGSzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agegnew/Birhan/blob/master/voice_command.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tjUZKdlJNr1"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29kte-jFKYU5"
      },
      "source": [
        "!pip install 'tensorflow-gpu==1.15.2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GWWfy1kKPv5",
        "outputId": "5c77d5ea-73a2-4e3f-8eac-2b2eab1f6119"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs sox libsox-fmt-mp3\n",
        "!git lfs install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
            "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox-fmt-mp3 libsox3\n",
            "  sox\n",
            "0 upgraded, 12 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 7,100 kB of archives.\n",
            "After this operation, 21.6 MB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 2.13.3 [6,229 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3ubuntu0.18.04.1 [15.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 7,100 kB in 1s (5,649 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 12.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 148496 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../02-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../03-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package git-lfs.\n",
            "Preparing to unpack .../04-git-lfs_2.13.3_amd64.deb ...\n",
            "Unpacking git-lfs (2.13.3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../05-libid3tag0_0.15.1b-13_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../06-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../07-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../09-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-mp3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../11-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
            "Setting up git-lfs (2.13.3) ...\n",
            "Git LFS initialized.\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA4ZW1VWKrwF",
        "outputId": "947d2f92-496e-45b3-e31a-bb4c752d1fff"
      },
      "source": [
        "!git clone https://github.com/mozilla/DeepSpeech --branch v0.10.0-alpha.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 23883, done.\u001b[K\n",
            "remote: Counting objects: 100% (420/420), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 23883 (delta 238), reused 357 (delta 211), pack-reused 23463\u001b[K\n",
            "Receiving objects: 100% (23883/23883), 49.49 MiB | 22.66 MiB/s, done.\n",
            "Resolving deltas: 100% (16372/16372), done.\n",
            "Note: checking out 'fcbd92d0d75beee36473aa44669fa330ca522cdc'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTU5iLWM9jV"
      },
      "source": [
        "os.chdir('/content/DeepSpeech')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUIkuV4FNFee",
        "outputId": "84f275af-fa02-4e62-ece1-7fbaa0edef91"
      },
      "source": [
        "!pip3 install pydub"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4rj61isK5Wu"
      },
      "source": [
        "import pandas as pd\n",
        "import pydub \n",
        "from pydub import AudioSegment"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9indyQcLBzA"
      },
      "source": [
        "from os.path import isfile, join\n",
        "from os import listdir"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_lTwUgrLD4z",
        "outputId": "d6c00b57-1b86-4508-8104-3ad82d430f4d"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'አቅዋርጥ'\n",
        "rootDir = \"/content/akuarit\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_akuarit = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_akuarit[\"wav_filename\"]= wav_filename_ls\n",
        "df_akuarit[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_akuarit[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 084.wav', '6.1_(new).wav', 'Voice 038_(new).wav', 'Voice 081.wav', 'Voice 014_(new).wav', 'Voice 015_(new).wav', 'Voice 087.wav', '7_(new).wav', 'Voice 083.wav', 'Voice 029_(new).wav', 'Voice 090.wav', 'Voice 023_(new).wav', 'Voice 089.wav', 'Voice 001_(new).wav', 'Voice 082.wav', 'Voice 037_(new).wav', 'Voice 088.wav', '6.4_(new).wav', '7.1_(new).wav', '6_(new).wav', '6.2_(new).wav', 'Voice 085.wav', 'Voice 016_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73MUtqvaLaE-"
      },
      "source": [
        "df_akuarit.to_csv('akuarit.csv', index= False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLDt1noie0Jy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test =train_test_split( df_akuarit, test_size= 0.1 , random_state=0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYrSUP5ie_U4"
      },
      "source": [
        "train.to_csv(\"aktrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"aktest.csv\", index= False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16N0luY6LcQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19aa7e8-4a82-4380-b035-0859d47fe0b1"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'ጀምር'\n",
        "rootDir = \"/content/jemir\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_jemir = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_jemir[\"wav_filename\"]= wav_filename_ls\n",
        "df_jemir[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_jemir[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 006.wav', 'Voice 005_(new).wav', 'Voice 004.wav', 'Voice 008_(new).wav', 'Voice 002.wav', 'Voice 033_(new).wav', 'gemir_(new).wav', 'Voice 021_(new).wav', 'Voice 008.wav', 'Voice 035_(new).wav', 'Voice 006_(new).wav', 'Voice 005.wav', 'Voice 003.wav', 'Voice 034_(new).wav', 'Voice 041_(new).wav', 'Voice 007_(new).wav', 'Voice 001_(new).wav', 'Voice 002_(new).wav', 'Voice 010.wav', 'Voice 004_(new).wav', 'Voice 025_(new).wav', 'Voice 003_(new).wav', 'Voice 007.wav', 'Voice 009.wav', 'Voice 027_(new).wav', 'Voice 001.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v03JE-PwNnky"
      },
      "source": [
        "df_jemir.to_csv('jemir.csv', index= False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io6idZVdfNND"
      },
      "source": [
        "train, test =train_test_split( df_jemir, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"jmtrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"jmtest.csv\", index= False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NK4qFC1LxK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1db991-40a9-4944-913a-14eef381fc2b"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'አስጀምር'\n",
        "rootDir = \"/content/asjemir\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_asjemir = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_asjemir[\"wav_filename\"]= wav_filename_ls\n",
        "df_asjemir[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_asjemir[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 015.wav', 'Voice 016.wav', 'Voice 018.wav', 'Voice 011.wav', '1.4_(new).wav', '1.3_(new).wav', 'Voice 012.wav', 'Voice 019.wav', 'Voice 014.wav', 'Voice 013.wav', 'Voice 020.wav', 'Voice 017.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXVfhjKYOGml"
      },
      "source": [
        "df_asjemir.to_csv('asjemir.csv', index= False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM9Rh2w_fd-v"
      },
      "source": [
        "train, test =train_test_split( df_asjemir, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"asjmtrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"asjmtest.csv\", index= False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0___FnHrL4QH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9c243d-7d02-4f75-d027-db2abacb52ff"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'አጫውት'\n",
        "rootDir = \"/content/achawet\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_achawet = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_achawet[\"wav_filename\"]= wav_filename_ls\n",
        "df_achawet[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_achawet[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 027.wav', '2_(new).wav', 'Voice 025.wav', 'Voice 021.wav', 'Voice 029.wav', 'Voice 001_sd_(new).wav', 'Voice 028.wav', 'Voice 024.wav', 'Voice 026.wav', 'Voice 022.wav', 'Voice 030.wav', 'Voice 023.wav', '1.5_(new).wav', '1.6_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCRE8z_vOneM"
      },
      "source": [
        "df_achawet.to_csv('achawet.csv', index= False)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcHyl6GjfmxC"
      },
      "source": [
        "train, test =train_test_split( df_achawet, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"achatrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"achatest.csv\", index= False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdoPqz0hMBAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e36cfc-4f4a-441d-efe2-11f2452c3075"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'አሳልፍ'\n",
        "rootDir = \"/content/asalif\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_asalif = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_asalif[\"wav_filename\"]= wav_filename_ls\n",
        "df_asalif[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_asalif[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 044.wav', 'Voice 049.wav', '2.3_(new).wav', '2.4_(new).wav', 'Voice 046.wav', 'Voice 042.wav', 'Voice 047.wav', 'Voice 050.wav', 'Voice 041.wav', '2.2_(new).wav', 'Voice 045.wav', 'Voice 048.wav', 'Voice 043.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Y8jdOCO2qa"
      },
      "source": [
        "df_asalif.to_csv('asalif.csv', index= False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88GijOw8ftxZ"
      },
      "source": [
        "train, test =train_test_split( df_asalif, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"asaltrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"asaltest.csv\", index= False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuhGV-HBMPNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce03ac44-2ea8-4b9a-cc77-0dbd26b1f153"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'መልስ'\n",
        "rootDir = \"/content/melis\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_melis = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_melis[\"wav_filename\"]= wav_filename_ls\n",
        "df_melis[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_melis[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 058.wav', 'Voice 057.wav', '3.4_(new).wav', 'Voice 060.wav', 'Voice 055.wav', 'Voice 052.wav', 'Voice 051.wav', 'Voice 053.wav', 'Voice 056.wav', 'Voice 054.wav', '3.2_(new).wav', '3_(new).wav', 'Voice 059.wav', '3.1_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_31PqEtdPGrZ"
      },
      "source": [
        "df_melis.to_csv('melis.csv', index= False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujMM12X6f8e8"
      },
      "source": [
        "train, test =train_test_split( df_melis, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"melstrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"melstest.csv\", index= False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjiCZ7ZbMXhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7912a3e6-5da3-4fe9-85bc-c3470daa45d2"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'አቁም'\n",
        "rootDir = \"/content/akum\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_akum = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_akum[\"wav_filename\"]= wav_filename_ls\n",
        "df_akum[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_akum[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 063.wav', '4_(new).wav', 'Voice 064.wav', '4.2_(new).wav', 'Voice 067.wav', 'Voice 068.wav', 'Voice 066.wav', 'Voice 065.wav', '4.4_(new).wav', 'Voice 032_(new).wav', 'Voice 069.wav', 'Voice 031_(new).wav', 'Voice 061.wav', 'Voice 070.wav', 'Voice 062.wav', '4.1_(new).wav', 'Voice 009_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KliTnefPb77"
      },
      "source": [
        "df_akum.to_csv('akum.csv', index= False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmxilVqDgEYF"
      },
      "source": [
        "train, test =train_test_split( df_akum, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"akumtrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"akumtest.csv\", index= False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJqwEvT0MdyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c164a9e-d504-4d79-ddc7-225aff25569d"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'ቀጥል'\n",
        "rootDir = \"/content/ketil\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_ketil = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_ketil[\"wav_filename\"]= wav_filename_ls\n",
        "df_ketil[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_ketil[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 080.wav', 'Voice 020_sd_(new).wav', 'Voice 074.wav', 'Voice 079.wav', 'Voice 076.wav', 'Voice 072.wav', 'Voice 018_sd_(new).wav', 'Voice 022_sd_(new).wav', 'Voice 073.wav', 'Voice 012_sd_(new).wav', 'Voice 013_sd_(new).wav', 'Voice 077.wav', 'Voice 078.wav', '5_(new).wav', '5.6_(new).wav', 'Voice 024_sd_(new).wav', 'Voice 019_sd_(new).wav', '5.5_(new).wav', '5.1_(new).wav', 'Voice 071.wav', 'Voice 075.wav', '5.4_(new).wav', 'Voice 017_sd_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR8kieobPmd3"
      },
      "source": [
        "df_ketil.to_csv('ketil.csv', index= False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj2E__g4gMUA"
      },
      "source": [
        "train, test =train_test_split( df_ketil, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"ketltrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"ketltest.csv\", index= False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCizqWRXMlSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe647c4-dd15-40e7-91e4-261969a5b37a"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'ቀጣይ'\n",
        "rootDir = \"/content/ketay\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_awird = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_awird[\"wav_filename\"]= wav_filename_ls\n",
        "df_awird[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_awird[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 031.wav', 'Voice 036.wav', 'Voice 035.wav', '2_(new).wav', 'Voice 017_(new).wav', 'Voice 015_(new).wav', 'Voice 010_(new).wav', 'Voice 033.wav', '2.1_(new).wav', 'Voice 032.wav', 'Voice 040.wav', 'Voice 001_sd_(new).wav', 'Voice 034.wav', 'Voice 011_(new).wav', 'Voice 038.wav', 'Voice 037.wav', 'Voice 039.wav', 'Voice 013_(new).wav', 'Voice 016_(new).wav', 'Voice 012_(new).wav', 'Voice 009_(new).wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhSmRKzfP09e"
      },
      "source": [
        "df_awird.to_csv('ketay.csv', index= False)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej19yLJggUer"
      },
      "source": [
        "train, test =train_test_split( df_awird, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"ketaytrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"ketaytest.csv\", index= False)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IXNHcU3MryW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f76457-f6da-4fe6-ec9e-4e0926f24c89"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'ጫን'\n",
        "rootDir = \"/content/chan\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_chan = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_chan[\"wav_filename\"]= wav_filename_ls\n",
        "df_chan[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_chan[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 104.wav', 'Voice 101.wav', 'Voice 109.wav', 'Voice 107.wav', 'Voice 108.wav', 'Voice 103.wav', 'Voice 102.wav', 'Voice 105.wav', 'Voice 110.wav', 'Voice 106.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nie8hfGwQFJ1"
      },
      "source": [
        "df_chan.to_csv('chan.csv', index= False)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TE270fugbM8"
      },
      "source": [
        "train, test =train_test_split( df_chan, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"chantrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"chantest.csv\", index= False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uftX1fQnM1gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e200fa-88cd-4dad-922e-ef8f280a1c90"
      },
      "source": [
        "wav_filename_ls = []\n",
        "wav_filesize_ls = []\n",
        "transcript_ls = []\n",
        "\n",
        "fname= 'ግዛ'\n",
        "rootDir = \"/content/giza\"\n",
        "\n",
        "trainDir = \"/content/trained\"\n",
        "\n",
        "\n",
        "df_giza = pd.DataFrame()\n",
        "count = 24\n",
        "path = rootDir\n",
        "fileList= [f for f in listdir(path) if isfile(join(path, f))]\n",
        "print(fileList)\n",
        "for i, fls in enumerate(fileList):\n",
        "  filename = path+ \"/\"+ fls\n",
        "  if os.path.isfile(filename):\n",
        "    new_filename= trainDir+\"/\"+fname+\".\"+ str(count)+ '.wav'\n",
        "\n",
        "    sound= pydub.AudioSegment.from_wav(filename)\n",
        "    new_sound =sound.set_frame_rate(16000).set_channels(1)\n",
        "    new_sound.export(new_filename, format=\"wav\")\n",
        "    count = count +1\n",
        "\n",
        "    wav_filesize = os.path.getsize(new_filename)\n",
        "    transcript = fname\n",
        "    \n",
        "    wav_filename_ls.append(new_filename)\n",
        "    wav_filesize_ls.append(wav_filesize)\n",
        "    transcript_ls.append(transcript)\n",
        "\n",
        "df_giza[\"wav_filename\"]= wav_filename_ls\n",
        "df_giza[\"wav_filesize\"]= wav_filesize_ls\n",
        "df_giza[\"transcript\"]= transcript_ls"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Voice 114.wav', 'Voice 120.wav', 'Voice 007_sd_(new).wav', 'Voice 006_sd_(new).wav', 'Voice 004_sd_(new).wav', 'Voice 118.wav', 'Voice 010_sd_(new).wav', 'Voice 008_sd_(new).wav', 'Voice 119.wav', 'Voice 117.wav', 'Voice 112.wav', 'Voice 003_sd_(new).wav', 'Voice 116.wav', 'Voice 005_sd_(new).wav', 'Voice 011_sd_(new).wav', 'Voice 115.wav', 'Voice 111.wav', 'Voice 002_sd_(new).wav', 'Voice 113.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsb0TbuzQSli"
      },
      "source": [
        "df_giza.to_csv('giza.csv', index= False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byU-eEnbgkcT"
      },
      "source": [
        "train, test =train_test_split( df_giza, test_size= 0.1 , random_state=0)\n",
        "train.to_csv(\"gizatrain.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"gizatest.csv\", index= False)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgavRe5dSbXn",
        "outputId": "3f26ab30-8ca9-4cd4-c373-923926fd5cb1"
      },
      "source": [
        "!pip install pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "_20E6zu9TKzD",
        "outputId": "8f83d53c-eb7d-43ab-f220-0306c23e4e40"
      },
      "source": [
        "!pip uninstall pandas\n",
        "!pip install pandas==1.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 1.3.2\n",
            "Uninstalling pandas-1.3.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas-1.3.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled pandas-1.3.2\n",
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 1.1.5\n",
            "Uninstalling pandas-1.1.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas-1.1.5.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled pandas-1.1.5\n",
            "Collecting pandas==1.1.5\n",
            "  Using cached pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4nzKHwtTWJQ",
        "outputId": "b8d089d4-3bab-47c2-e53f-108b24512af4"
      },
      "source": [
        "!pip install CompressionOptions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement CompressionOptions (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for CompressionOptions\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqL8LRZQLctR"
      },
      "source": [
        "df_notification.to_csv('akuarit.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjfLP_9QLfw7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test =train_test_split( df_notification, test_size= 0.1 , random_state=0)\n",
        "#dev, test =train_test_split( test, test_size= 0.5 , random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Z_mjwvLiWh",
        "outputId": "bd9ec495-7623-49a4-8362-4101d7b89ac9"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbqp84YSLjQg"
      },
      "source": [
        "train.to_csv(\"train.csv\", index= False)\n",
        "#dev.to_csv(\"dev.csv\", index= False)\n",
        "test.to_csv(\"test.csv\", index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "nnF-DmjlLt6l",
        "outputId": "a6911ea3-8705-43b8-ea6b-ffffb1e0b6f0"
      },
      "source": [
        "import soundfile\n",
        "\n",
        "def convert(wavfilename):\n",
        "  data, samplerate =soundfile.read(wavfilename)\n",
        "  soundfile.write(wavfilename, data, samplerate, subtype= 'PCM_16')\n",
        "  return wav_filename_ls\n",
        "\n",
        "train['wav_filename'] = train['wav_filename'].apply(convert)\n",
        "dev['wav_filename'] = dev['wav_filename'].apply(convert)\n",
        "test['wav_filename'] = test['wav_filename'].apply(convert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a980a4c957ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wav_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2u9-c3JLyTq",
        "outputId": "e2b92113-0f8f-423e-bf18-9815cc575822"
      },
      "source": [
        "!python3 DeepSpeech.py --n_hidden 2048 --checkpoint_dir ./modelcheck --save_checkpoint_dir ./modelcheck2 --epochs 5 --train_files train.csv --test_files test.csv --learning_rate 0.0001 --noearly_stop --export_dir outmodel/  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W WARNING: You specified different values for --load_checkpoint_dir and --save_checkpoint_dir, but you are running training and testing in a single invocation. The testing step will respect --load_checkpoint_dir, and thus WILL NOT TEST THE CHECKPOINT CREATED BY THE TRAINING STEP. Train and test in two separate invocations, specifying the correct --load_checkpoint_dir in both cases, or use the same location for loading and saving.\n",
            "I Could not find best validating checkpoint.\n",
            "I Loading most recent checkpoint from ./modelcheck/train-240\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:01:50 | Steps: 8 | Loss: 0.831238        \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 1 |   Training | Elapsed Time: 0:01:53 | Steps: 8 | Loss: 0.753153        \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 |   Training | Elapsed Time: 0:01:55 | Steps: 8 | Loss: 0.642786        \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 |   Training | Elapsed Time: 0:01:55 | Steps: 8 | Loss: 0.832161        \n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 |   Training | Elapsed Time: 0:01:47 | Steps: 8 | Loss: 0.899499        \n",
            "--------------------------------------------------------------------------------\n",
            "I FINISHED optimization in 0:09:30.436465\n",
            "I Could not find best validating checkpoint.\n",
            "I Loading most recent checkpoint from ./modelcheck/train-240\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on test.csv\n",
            "Test epoch | Steps: 1 | Elapsed Time: 0:00:01                                   \n",
            "Test on test.csv - WER: 0.000000, CER: 0.000000, loss: 0.913327\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.913327\n",
            " - wav: file:///content/trained/akuarit.31.wav\n",
            " - src: \"akuarit\"\n",
            " - res: \"akuarit\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.913327\n",
            " - wav: file:///content/trained/akuarit.31.wav\n",
            " - src: \"akuarit\"\n",
            " - res: \"akuarit\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.000000, CER: 0.000000, loss: 0.913327\n",
            " - wav: file:///content/trained/akuarit.31.wav\n",
            " - src: \"akuarit\"\n",
            " - res: \"akuarit\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Could not find best validating checkpoint.\n",
            "I Loading most recent checkpoint from ./modelcheck/train-240\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at outmodel/\n",
            "I Model metadata file saved to outmodel/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdulWQm6wId5",
        "outputId": "7a97ed63-72ab-4b7b-c4db-fe623d9dac8a"
      },
      "source": [
        "!pip3 install deepspeech"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeech\n",
            "  Downloading deepspeech-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (9.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.2 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from deepspeech) (1.18.5)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAB-mmKhxRRe",
        "outputId": "5b6a3608-76f5-4160-bed2-5d2a41de9655"
      },
      "source": [
        "!pip install sox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sox in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from sox) (1.18.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHjPdZrsx40R"
      },
      "source": [
        "import sys, wave, math\n",
        "import numpy as np\n",
        "\n",
        "wave_data = np.zeros(44100).astype(np.short)\n",
        "\n",
        "f = wave.open('Voice6.wav', 'wb')\n",
        "f.setnchannels(1)\n",
        "f.setsampwidth(2)\n",
        "f.setframerate(96000)\n",
        "f.writeframes(wave_data.tostring())\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX9TuxDnMIRQ",
        "outputId": "61fcabff-bfe6-49a9-f8b1-1b0312c23161"
      },
      "source": [
        "!deepspeech --model outmodel/output_graph.pb --audio Voice6.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from file outmodel/output_graph.pb\n",
            "TensorFlow: v2.3.0-6-g23ad988\n",
            "DeepSpeech: v0.9.3-0-gf2e9c85\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2021-09-04 21:53:57.757497: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded model in 0.604s.\n",
            "Warning: original sample rate (96000) is different than 16000hz. Resampling might produce erratic speech recognition.\n",
            "Running inference.\n",
            "ak\n",
            "Inference took 4.040s for 0.459s audio file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xpzInh0YNiX5",
        "outputId": "e9dfabdf-bc4d-4eba-c499-2451773c66c4"
      },
      "source": [
        "!pip3 install --upgrade --force-reinstall -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/DeepSpeech\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting progressbar2\n",
            "  Using cached progressbar2-3.53.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pyxdg\n",
            "  Using cached pyxdg-0.27-py2.py3-none-any.whl (49 kB)\n",
            "Collecting attrdict\n",
            "  Using cached attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting absl-py\n",
            "  Using cached absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
            "Collecting semver\n",
            "  Using cached semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Using cached opuslib-2.0.0-py3-none-any.whl\n",
            "Collecting optuna\n",
            "  Using cached optuna-2.9.1-py3-none-any.whl (302 kB)\n",
            "Collecting sox\n",
            "  Using cached sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting bs4\n",
            "  Using cached bs4-0.0.1-py3-none-any.whl\n",
            "Collecting pandas\n",
            "  Using cached pandas-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "Collecting numba==0.47.0\n",
            "  Using cached numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "Collecting llvmlite==0.31.0\n",
            "  Using cached llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "Collecting librosa\n",
            "  Using cached librosa-0.8.1-py3-none-any.whl (203 kB)\n",
            "Collecting soundfile\n",
            "  Using cached SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder==0.10.0-alpha.3\n",
            "  Using cached ds_ctcdecoder-0.10.0a3-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Collecting tensorflow==1.15.4\n",
            "  Using cached tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-57.5.0-py3-none-any.whl (819 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Collecting astor>=0.6.0\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "Collecting wheel>=0.26\n",
            "  Using cached wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting protobuf>=3.6.1\n",
            "  Using cached protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting wrapt>=1.11.1\n",
            "  Using cached wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Using cached grpcio-1.39.0-cp37-cp37m-manylinux2014_x86_64.whl (4.3 MB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Collecting gast==0.2.2\n",
            "  Using cached gast-0.2.2-py3-none-any.whl\n",
            "Collecting h5py\n",
            "  Using cached h5py-3.4.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Using cached soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
            "Collecting cached-property\n",
            "  Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Using cached zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting resampy>=0.2.2\n",
            "  Using cached resampy-0.2.2-py3-none-any.whl\n",
            "Collecting joblib>=0.14\n",
            "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "Collecting scipy>=1.0.0\n",
            "  Using cached scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "Collecting pooch>=1.0\n",
            "  Using cached pooch-1.5.1-py3-none-any.whl (57 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-21.0-py3-none-any.whl (40 kB)\n",
            "Collecting decorator>=3.0.0\n",
            "  Using cached decorator-5.0.9-py3-none-any.whl (8.9 kB)\n",
            "Collecting audioread>=2.0.0\n",
            "  Using cached audioread-2.1.9-py3-none-any.whl\n",
            "Collecting scikit-learn!=0.19.0,>=0.14.0\n",
            "  Using cached scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "Collecting pyparsing>=2.0.2\n",
            "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Collecting appdirs\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting cffi>=1.0\n",
            "  Using cached cffi-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Using cached cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting PyYAML\n",
            "  Using cached PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting cliff\n",
            "  Using cached cliff-3.9.0-py3-none-any.whl (80 kB)\n",
            "Collecting alembic\n",
            "  Using cached alembic-1.7.1-py3-none-any.whl (208 kB)\n",
            "Collecting sqlalchemy>=1.1.0\n",
            "  Using cached SQLAlchemy-1.4.23-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
            "Collecting colorlog\n",
            "  Using cached colorlog-6.4.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting greenlet!=0.4.17\n",
            "  Using cached greenlet-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Collecting importlib-resources\n",
            "  Using cached importlib_resources-5.2.2-py3-none-any.whl (27 kB)\n",
            "Collecting Mako\n",
            "  Using cached Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Using cached pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Using cached stevedore-3.4.0-py3-none-any.whl (49 kB)\n",
            "Collecting PrettyTable>=0.7.2\n",
            "  Using cached prettytable-2.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting autopage>=0.4.0\n",
            "  Using cached autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Using cached cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "Collecting colorama>=0.3.7\n",
            "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting wcwidth>=0.1.7\n",
            "  Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting attrs>=16.3.0\n",
            "  Using cached attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Using cached pyperclip-1.8.2-py3-none-any.whl\n",
            "Collecting MarkupSafe>=0.9.2\n",
            "  Using cached MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Collecting python-dateutil>=2.7.3\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
            "Collecting python-utils>=2.3.0\n",
            "  Using cached python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.2-py3-none-any.whl (59 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Using cached charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "Installing collected packages: zipp, typing-extensions, wcwidth, urllib3, six, setuptools, pyperclip, pyparsing, pycparser, pbr, numpy, MarkupSafe, llvmlite, importlib-metadata, idna, greenlet, colorama, charset-normalizer, certifi, cached-property, attrs, wheel, werkzeug, threadpoolctl, stevedore, sqlalchemy, soupsieve, scipy, requests, PyYAML, protobuf, PrettyTable, packaging, numba, markdown, Mako, joblib, importlib-resources, h5py, grpcio, cmd2, cffi, autopage, appdirs, absl-py, wrapt, tqdm, termcolor, tensorflow-estimator, tensorboard, soundfile, scikit-learn, resampy, pytz, python-utils, python-dateutil, pooch, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, decorator, colorlog, cmaes, cliff, beautifulsoup4, audioread, astor, alembic, tensorflow, sox, semver, pyxdg, progressbar2, pandas, opuslib, optuna, librosa, ds-ctcdecoder, bs4, attrdict, deepspeech-training\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.5.0\n",
            "    Uninstalling zipp-3.5.0:\n",
            "      Successfully uninstalled zipp-3.5.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.6\n",
            "    Uninstalling urllib3-1.26.6:\n",
            "      Successfully uninstalled urllib3-1.26.6\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.5.0\n",
            "    Uninstalling setuptools-57.5.0:\n",
            "      Successfully uninstalled setuptools-57.5.0\n",
            "  Attempting uninstall: pyperclip\n",
            "    Found existing installation: pyperclip 1.8.2\n",
            "    Uninstalling pyperclip-1.8.2:\n",
            "      Successfully uninstalled pyperclip-1.8.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: pbr\n",
            "    Found existing installation: pbr 5.6.0\n",
            "    Uninstalling pbr-5.6.0:\n",
            "      Successfully uninstalled pbr-5.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.1\n",
            "    Uninstalling importlib-metadata-4.8.1:\n",
            "      Successfully uninstalled importlib-metadata-4.8.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.2\n",
            "    Uninstalling idna-3.2:\n",
            "      Successfully uninstalled idna-3.2\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 1.1.1\n",
            "    Uninstalling greenlet-1.1.1:\n",
            "      Successfully uninstalled greenlet-1.1.1\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.4\n",
            "    Uninstalling colorama-0.4.4:\n",
            "      Successfully uninstalled colorama-0.4.4\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.4\n",
            "    Uninstalling charset-normalizer-2.0.4:\n",
            "      Successfully uninstalled charset-normalizer-2.0.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.5.30\n",
            "    Uninstalling certifi-2021.5.30:\n",
            "      Successfully uninstalled certifi-2021.5.30\n",
            "  Attempting uninstall: cached-property\n",
            "    Found existing installation: cached-property 1.5.2\n",
            "    Uninstalling cached-property-1.5.2:\n",
            "      Successfully uninstalled cached-property-1.5.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.37.0\n",
            "    Uninstalling wheel-0.37.0:\n",
            "      Successfully uninstalled wheel-0.37.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 2.0.1\n",
            "    Uninstalling Werkzeug-2.0.1:\n",
            "      Successfully uninstalled Werkzeug-2.0.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 2.2.0\n",
            "    Uninstalling threadpoolctl-2.2.0:\n",
            "      Successfully uninstalled threadpoolctl-2.2.0\n",
            "  Attempting uninstall: stevedore\n",
            "    Found existing installation: stevedore 3.4.0\n",
            "    Uninstalling stevedore-3.4.0:\n",
            "      Successfully uninstalled stevedore-3.4.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.23\n",
            "    Uninstalling SQLAlchemy-1.4.23:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.23\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.2.1\n",
            "    Uninstalling soupsieve-2.2.1:\n",
            "      Successfully uninstalled soupsieve-2.2.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.1\n",
            "    Uninstalling scipy-1.7.1:\n",
            "      Successfully uninstalled scipy-1.7.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.26.0\n",
            "    Uninstalling requests-2.26.0:\n",
            "      Successfully uninstalled requests-2.26.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 2.2.0\n",
            "    Uninstalling prettytable-2.2.0:\n",
            "      Successfully uninstalled prettytable-2.2.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.0\n",
            "    Uninstalling packaging-21.0:\n",
            "      Successfully uninstalled packaging-21.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.47.0\n",
            "    Uninstalling numba-0.47.0:\n",
            "      Successfully uninstalled numba-0.47.0\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.3.4\n",
            "    Uninstalling Markdown-3.3.4:\n",
            "      Successfully uninstalled Markdown-3.3.4\n",
            "  Attempting uninstall: Mako\n",
            "    Found existing installation: Mako 1.1.5\n",
            "    Uninstalling Mako-1.1.5:\n",
            "      Successfully uninstalled Mako-1.1.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.0.1\n",
            "    Uninstalling joblib-1.0.1:\n",
            "      Successfully uninstalled joblib-1.0.1\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib-resources 5.2.2\n",
            "    Uninstalling importlib-resources-5.2.2:\n",
            "      Successfully uninstalled importlib-resources-5.2.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.4.0\n",
            "    Uninstalling h5py-3.4.0:\n",
            "      Successfully uninstalled h5py-3.4.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.39.0\n",
            "    Uninstalling grpcio-1.39.0:\n",
            "      Successfully uninstalled grpcio-1.39.0\n",
            "  Attempting uninstall: cmd2\n",
            "    Found existing installation: cmd2 2.1.2\n",
            "    Uninstalling cmd2-2.1.2:\n",
            "      Successfully uninstalled cmd2-2.1.2\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.6\n",
            "    Uninstalling cffi-1.14.6:\n",
            "      Successfully uninstalled cffi-1.14.6\n",
            "  Attempting uninstall: autopage\n",
            "    Found existing installation: autopage 0.4.0\n",
            "    Uninstalling autopage-0.4.0:\n",
            "      Successfully uninstalled autopage-0.4.0\n",
            "  Attempting uninstall: appdirs\n",
            "    Found existing installation: appdirs 1.4.4\n",
            "    Uninstalling appdirs-1.4.4:\n",
            "      Successfully uninstalled appdirs-1.4.4\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.13.0\n",
            "    Uninstalling absl-py-0.13.0:\n",
            "      Successfully uninstalled absl-py-0.13.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.2\n",
            "    Uninstalling tqdm-4.62.2:\n",
            "      Successfully uninstalled tqdm-4.62.2\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Attempting uninstall: soundfile\n",
            "    Found existing installation: SoundFile 0.10.3.post1\n",
            "    Uninstalling SoundFile-0.10.3.post1:\n",
            "      Successfully uninstalled SoundFile-0.10.3.post1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2021.1\n",
            "    Uninstalling pytz-2021.1:\n",
            "      Successfully uninstalled pytz-2021.1\n",
            "  Attempting uninstall: python-utils\n",
            "    Found existing installation: python-utils 2.5.6\n",
            "    Uninstalling python-utils-2.5.6:\n",
            "      Successfully uninstalled python-utils-2.5.6\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pooch\n",
            "    Found existing installation: pooch 1.5.1\n",
            "    Uninstalling pooch-1.5.1:\n",
            "      Successfully uninstalled pooch-1.5.1\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.3.0\n",
            "    Uninstalling opt-einsum-3.3.0:\n",
            "      Successfully uninstalled opt-einsum-3.3.0\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: keras-applications\n",
            "    Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 5.0.9\n",
            "    Uninstalling decorator-5.0.9:\n",
            "      Successfully uninstalled decorator-5.0.9\n",
            "  Attempting uninstall: colorlog\n",
            "    Found existing installation: colorlog 6.4.1\n",
            "    Uninstalling colorlog-6.4.1:\n",
            "      Successfully uninstalled colorlog-6.4.1\n",
            "  Attempting uninstall: cmaes\n",
            "    Found existing installation: cmaes 0.8.2\n",
            "    Uninstalling cmaes-0.8.2:\n",
            "      Successfully uninstalled cmaes-0.8.2\n",
            "  Attempting uninstall: cliff\n",
            "    Found existing installation: cliff 3.9.0\n",
            "    Uninstalling cliff-3.9.0:\n",
            "      Successfully uninstalled cliff-3.9.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.9.3\n",
            "    Uninstalling beautifulsoup4-4.9.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.9.3\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 2.1.9\n",
            "    Uninstalling audioread-2.1.9:\n",
            "      Successfully uninstalled audioread-2.1.9\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: alembic\n",
            "    Found existing installation: alembic 1.7.1\n",
            "    Uninstalling alembic-1.7.1:\n",
            "      Successfully uninstalled alembic-1.7.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.15.4\n",
            "    Uninstalling tensorflow-1.15.4:\n",
            "      Successfully uninstalled tensorflow-1.15.4\n",
            "  Attempting uninstall: sox\n",
            "    Found existing installation: sox 1.4.1\n",
            "    Uninstalling sox-1.4.1:\n",
            "      Successfully uninstalled sox-1.4.1\n",
            "  Attempting uninstall: semver\n",
            "    Found existing installation: semver 2.13.0\n",
            "    Uninstalling semver-2.13.0:\n",
            "      Successfully uninstalled semver-2.13.0\n",
            "  Attempting uninstall: pyxdg\n",
            "    Found existing installation: pyxdg 0.27\n",
            "    Uninstalling pyxdg-0.27:\n",
            "      Successfully uninstalled pyxdg-0.27\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.53.1\n",
            "    Uninstalling progressbar2-3.53.1:\n",
            "      Successfully uninstalled progressbar2-3.53.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: opuslib\n",
            "    Found existing installation: opuslib 2.0.0\n",
            "    Uninstalling opuslib-2.0.0:\n",
            "      Successfully uninstalled opuslib-2.0.0\n",
            "  Attempting uninstall: optuna\n",
            "    Found existing installation: optuna 2.9.1\n",
            "    Uninstalling optuna-2.9.1:\n",
            "      Successfully uninstalled optuna-2.9.1\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "  Attempting uninstall: ds-ctcdecoder\n",
            "    Found existing installation: ds-ctcdecoder 0.10.0a3\n",
            "    Uninstalling ds-ctcdecoder-0.10.0a3:\n",
            "      Successfully uninstalled ds-ctcdecoder-0.10.0a3\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.1\n",
            "    Uninstalling bs4-0.0.1:\n",
            "      Successfully uninstalled bs4-0.0.1\n",
            "  Attempting uninstall: attrdict\n",
            "    Found existing installation: attrdict 2.0.1\n",
            "    Uninstalling attrdict-2.0.1:\n",
            "      Successfully uninstalled attrdict-2.0.1\n",
            "  Attempting uninstall: deepspeech-training\n",
            "    Found existing installation: deepspeech-training 0.10.0a3\n",
            "    Can't uninstall 'deepspeech-training'. No files were found to uninstall.\n",
            "  Running setup.py develop for deepspeech-training\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n",
            "moviepy 0.2.3.5 requires decorator<5.0,>=4.0.2, but you have decorator 5.0.9 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Mako-1.1.5 MarkupSafe-2.0.1 PrettyTable-2.2.0 PyYAML-5.4.1 absl-py-0.13.0 alembic-1.7.1 appdirs-1.4.4 astor-0.8.1 attrdict-2.0.1 attrs-21.2.0 audioread-2.1.9 autopage-0.4.0 beautifulsoup4-4.9.3 bs4-0.0.1 cached-property-1.5.2 certifi-2021.5.30 cffi-1.14.6 charset-normalizer-2.0.4 cliff-3.9.0 cmaes-0.8.2 cmd2-2.1.2 colorama-0.4.4 colorlog-6.4.1 decorator-5.0.9 deepspeech-training-0.10.0a3 ds-ctcdecoder-0.10.0a3 gast-0.2.2 google-pasta-0.2.0 greenlet-1.1.1 grpcio-1.39.0 h5py-3.4.0 idna-3.2 importlib-metadata-4.8.1 importlib-resources-5.2.2 joblib-1.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 librosa-0.8.1 llvmlite-0.31.0 markdown-3.3.4 numba-0.47.0 numpy-1.18.5 opt-einsum-3.3.0 optuna-2.9.1 opuslib-2.0.0 packaging-21.0 pandas-1.3.2 pbr-5.6.0 pooch-1.5.1 progressbar2-3.53.1 protobuf-3.17.3 pycparser-2.20 pyparsing-2.4.7 pyperclip-1.8.2 python-dateutil-2.8.2 python-utils-2.5.6 pytz-2021.1 pyxdg-0.27 requests-2.26.0 resampy-0.2.2 scikit-learn-0.24.2 scipy-1.7.1 semver-2.13.0 setuptools-57.5.0 six-1.16.0 soundfile-0.10.3.post1 soupsieve-2.2.1 sox-1.4.1 sqlalchemy-1.4.23 stevedore-3.4.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1 termcolor-1.1.0 threadpoolctl-2.2.0 tqdm-4.62.2 typing-extensions-3.10.0.2 urllib3-1.26.6 wcwidth-0.2.5 werkzeug-2.0.1 wheel-0.37.0 wrapt-1.12.1 zipp-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "astor",
                  "cached_property",
                  "certifi",
                  "cffi",
                  "charset_normalizer",
                  "colorama",
                  "dateutil",
                  "decorator",
                  "gast",
                  "google",
                  "h5py",
                  "idna",
                  "joblib",
                  "keras_applications",
                  "keras_preprocessing",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "pyparsing",
                  "pytz",
                  "requests",
                  "scipy",
                  "six",
                  "sklearn",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator",
                  "termcolor",
                  "typing_extensions",
                  "urllib3",
                  "wcwidth",
                  "wrapt",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}